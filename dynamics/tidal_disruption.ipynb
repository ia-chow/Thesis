{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2acff0a0-25b7-44f5-97c9-70353fe3f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KernelDensity  # kernel density estimation\n",
    "from mpmath import jtheta  # jacobi elliptic theta function\n",
    "import multiprocessing\n",
    "from sbpy.data import Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b94157-0cc0-4149-9032-be3636f8446e",
   "metadata": {},
   "source": [
    "Import list of all minor planets from the MPC `MPCORB` `.json` file so we can use the same data set as Granvik 2016, 2017, 2018:\n",
    "\n",
    "Note that the `.json` file needs to be gotten from the MPC for local use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a87eda-f10e-47f1-b829-0d3adfb6f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all minor planets from the MPC:\n",
    "all_mps = pd.read_json('mpcorb_extended.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4fe79a-2d72-4e8e-ba47-754620ffd26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp_colspecs = [(0, 7), (8, 13), (14, 19), (20, 25), (26, 35), (37, 46), \n",
    "#                (48, 57), (59, 68), (70, 79), (80, 91), (92, 103), (105, 106), \n",
    "#                (107, 116), (117, 122), (123, 126), (127, 136), \n",
    "#                (137, 141), (132, 145), (146, 149), (150, 160), (151, 165), (166, 194), (194, 202)]\n",
    "\n",
    "# mp_columns = ['Number/Principal_desig', 'H', 'G', 'Epoch', \n",
    "#              'M', 'Peri', 'Node', 'i', 'e', 'n', 'a', 'U', \n",
    "#              'Ref', 'Num_obs', 'Num_opps', 'Arc_length/Arc_years', \n",
    "#              'rms', 'Perturbers', 'Perturbers_2', 'Computer', 'flags', 'Other_desigs', 'Date']\n",
    "\n",
    "# all_mps = pd.read_fwf('MPCORB.DAT', skiprows=43, colspecs=mp_colspecs, header=None)\n",
    "# all_mps.columns = mp_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9cf85c-3029-423c-a47b-8a83107de009",
   "metadata": {},
   "source": [
    "Import list of all comets from the MPC as well for completeness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c97189b-3a71-4631-8767-5662b459a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "comet_colspecs = [(0, 4), (4, 5), (5, 12), (14, 18), (19, 21), (22, 29), (30, 39), (41, 49), (51, 59), (61, 69), \n",
    "                  (71, 79), (81, 85), (85, 87), \n",
    "                  (87, 89), (91, 95), (96, 100), (102, 158), (159, 168)]\n",
    "\n",
    "comet_columns = ['number', 'orbit_type', 'prov_desig', 'year', 'month', 'day', \n",
    "                      'q', 'e', 'arg_peri', 'node', 'inc', \n",
    "                      'year_epoch', 'month_epoch', 'day_epoch', 'H', 'slope', 'designation', 'reference']\n",
    "\n",
    "all_comets = pd.read_fwf('CometEls.txt', colspecs=comet_colspecs, header=None)\n",
    "# columns\n",
    "# orbit format is https://www.minorplanetcenter.net/iau/info/CometOrbitFormat.html\n",
    "all_comets.columns = comet_columns\n",
    "all_comets['H'] = pd.to_numeric(all_comets.H, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b62c34-9c67-426a-975f-7662fdcf04d1",
   "metadata": {},
   "source": [
    "Read in all the `.csv` files containing all numbered and unnumbered minor planets, as well as comets, observed by either `G96` or `703` (CSS) between $2005$ and $2012$. These are generated by processing the `NumObs.txt`, `UnnObs.txt` and `CmtObs.txt` files from the MPC with the `mpc_process.py`, `mpc_process_unn.py` and `mpc_process_cmt.py` scripts, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c57afb-f10a-4f18-97d3-4baad2fe610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_css_num_mpcs = set()\n",
    "all_css_unn_mpcs = set()\n",
    "# two sets for the comets, one for those identified by number and one for those identified by provisional designation\n",
    "all_css_cmts_numbered = set()\n",
    "all_css_cmts_prov_desig = set()\n",
    "\n",
    "# read in files in a loop\n",
    "for i in tqdm(range(0, 426)):\n",
    "    processed_num_csv = pd.read_csv(f'mpc_data_num/mpc_processed_num_{i}.csv')\n",
    "    all_css_num_mpcs.update(processed_num_csv.packed_number)\n",
    "\n",
    "for j in tqdm(range(0, 36)):\n",
    "    processed_unn_csv = pd.read_csv(f'mpc_data_unn/mpc_processed_unn_{j}.csv')\n",
    "    all_css_unn_mpcs.update(processed_unn_csv.packed_desig)\n",
    "\n",
    "for k in tqdm(range(0, 2)):\n",
    "    processed_cmt_csv = pd.read_csv(f'mpc_data_cmt/mpc_processed_cmt_{k}.csv')\n",
    "    all_css_cmts_numbered.update(processed_cmt_csv.number.dropna())\n",
    "    all_css_cmts_prov_desig.update(processed_cmt_csv.provisional_desig.dropna())\n",
    "\n",
    "# convert from packed to unpacked format\n",
    "unpacked_css_num_mpcs = [Names.from_packed(str(mp)) for mp in list(all_css_num_mpcs)]\n",
    "unpacked_css_unn_mpcs = [Names.from_packed(str(mp.rstrip('*'))) for mp in list(all_css_unn_mpcs)]  # strip asterisk if it's there\n",
    "unpacked_css_cmts_numbered = [Names.from_packed(str(cmt)) for cmt in list(all_css_cmts_numbered)]\n",
    "unpacked_css_cmts_prov_desig = [Names.from_packed(str(cmt)) for cmt in list(all_css_cmts_prov_desig)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaedda8-abfb-4799-9c81-1c32d3c84634",
   "metadata": {},
   "source": [
    "Get the minor planet number and compute $q$ as $q = a\\left(1 - e\\right)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d8f101-ce88-4900-b5e0-e1d3f3d57b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mps['mp_number'] = all_mps.index.astype(int)  # index is the minor planet number\n",
    "# compute qs\n",
    "all_mps['q'] = (all_mps.a * (1. - all_mps.e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c126ccc5-4973-4e1f-baf9-d6005026d65e",
   "metadata": {},
   "source": [
    "Now filter the CSS minor planets using `MPCORB.dat` for $17 < H < 25$ (magnitude limited by Granvik's model) and $q < 1.3$ (NEO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571ac5c-12ca-4221-8d82-40eeac64ae5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get all numbered or unnumbered minor planets in mpcorb observed by CSS\n",
    "# this does not double count for sure!\n",
    "filtered_mps = all_mps[all_mps.mp_number.isin(unpacked_css_num_mpcs) | all_mps.Principal_desig.isin(unpacked_css_unn_mpcs)]\n",
    "# then filter by 17 < H < 25 and q < 1.3 to get the final list of NEOs:\n",
    "mag_limited_neos = filtered_mps[(17. < filtered_mps.H) & (filtered_mps.H < 25.) & (filtered_mps.q < 1.3)]\n",
    "len(mag_limited_neos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e44659-d020-4af5-8840-531c45f8bab0",
   "metadata": {},
   "source": [
    "$2412$ NEAs in the numbered list, adding the unnumbered list gives an additional $3155$ NEAs for $5567$ in total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f72862c-2719-4e9f-afb5-7e19162c2593",
   "metadata": {},
   "source": [
    "Filter the CSS comets in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920fce8-bc08-4dfd-9d43-7be67be3dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get all numbered or unnumbered comets in mpcorb observed by CSS\n",
    "filtered_comets = all_comets[all_comets.number.isin(unpacked_css_cmts_numbered) | all_comets.prov_desig.isin(unpacked_css_cmts_prov_desig)]\n",
    "# then filter by 17 < H < 25 and q < 1.3\n",
    "mag_limited_comets = filtered_comets[(17. < filtered_comets.H) & (filtered_comets.H < 25.) & (filtered_comets.q < 1.3)]\n",
    "len(mag_limited_comets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890ef01a-b4c8-498f-85cc-023af2b5d393",
   "metadata": {},
   "source": [
    "$1$ comet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aae48c6-7369-496e-af7a-8bc6e5baae05",
   "metadata": {},
   "source": [
    "Combine the minor planet and comet lists to get the full neo list and get their $q$ values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1866d287-bb13-407b-aae6-70caa2b147e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_limited_qs = np.concatenate((mag_limited_neos.q, mag_limited_comets.q))\n",
    "mag_limited_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1498fc8a-3018-4bdf-921b-d7cf23c6d8da",
   "metadata": {},
   "source": [
    "Importing the Monte Carlo cloned impactors and computing their $q$ (perihelion distance) values from $a$ and $e$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b6911e-5f3f-4768-b470-aed833cedae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "orb_param_array_all_events = np.load('orb_params_all_events_mc.npy')  # get the orbital parameters for the clones\n",
    "orb_param_array_true = np.load('true_impactor_aeis.npy')  # and for the true 14 impactors\n",
    "orb_param_variables = ['a', 'e', 'i', 'peri', 'node', 'M']  \n",
    "\n",
    "# get orbital param variables from the numpy files\n",
    "impactor_as, impactor_es, impactor_is, impactor_peris, impactor_nodes, impactor_Ms = orb_param_array_all_events.T\n",
    "true_as, true_es, true_is, true_peris, true_nodes, true_Ms = orb_param_array_true.T\n",
    "\n",
    "# compute q from q = a * (1. - e)\n",
    "impactor_qs = (impactor_as * (1. - impactor_es)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeedce28-4369-4913-8d10-9a675a02c2a8",
   "metadata": {},
   "source": [
    "Plotting un-weighted normalized histograms for the $q$ values of the cloned impactors and the CSS NEOs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423bbdb6-3d93-45fb-9162-c1c8ff908736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semi major axes of earth and venus\n",
    "a_earth = 1.\n",
    "a_venus = 0.72333199\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.hist(impactor_qs_flat, bins='auto', color = 'orange', label = 'Unweighted Impactor', density=True, alpha=0.9)\n",
    "plt.hist(mag_limited_qs, bins='auto', color='dodgerblue', label = 'CSS', density=True, alpha=0.7)\n",
    "# plt.yscale('log')\n",
    "plt.axvline(a_earth, color = 'k')\n",
    "plt.axvline(a_venus, color = 'k')\n",
    "plt.xlim(left=0., right=1.3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1b085-87a2-40bf-8f42-d2b859f7b732",
   "metadata": {},
   "source": [
    "Compare using a K-S test and find the $p$-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38cebad-bea6-4884-9968-462f37f61616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "scipy.stats.ks_2samp(impactor_qs_flat, mag_limited_qs).pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e656a-335b-4747-be2f-a6988a7d6aa4",
   "metadata": {},
   "source": [
    "Now compute weights for each of the impactors from their Opik impact probability, given $a$, $e$ and $i$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d611bb-dcfc-42bf-a452-cbf67c103bd2",
   "metadata": {},
   "source": [
    "Use the same function for collision probability per revolution in the `monte_carlo_granvik_model.ipynb` notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d35eb1-b640-45c7-815f-7265f9adc21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants for earth and sun\n",
    "a0 = 1.  # semi-major axis of earth in au\n",
    "m = 5.97e24  # mass of earth in kg\n",
    "R = 4.259e-5  # radius of earth in au\n",
    "M = 1.988e30  # mass of sun in kg\n",
    "\n",
    "# get the probability per revolution P\n",
    "def get_P(a, e, i, a0=a0, m=m, R=R, M=M):\n",
    "    \"\"\"\n",
    "    Get the collision probability per revolution P for an object with semi-major axis a in AU, eccentricity e, and inclination i in degrees\n",
    "    and a target on a circular orbit with constant semi-major axis a0 in AU, mass m in kg, and radius R in AU, \n",
    "    both orbiting a star with mass M in kg\n",
    "\n",
    "    return: P, the collision probability per revolution\n",
    "    \"\"\"\n",
    "    Q = R/a0  # Q\n",
    "    Ux = np.sqrt(2 - (1/a) - (a * (1 - (e ** 2))))  # Ux\n",
    "    U = np.sqrt(3 - (1/a) - ((2 * np.sqrt(a * (1 - e ** 2))) * np.cos(np.deg2rad(i))))  # U\n",
    "    tau = Q * np.sqrt(1 + ((2 * m)/(M * Q * (U ** 2))))  # tau\n",
    "    # tau = R * np.sqrt(1 + R/Q)\n",
    "    # compute P\n",
    "    P = ((tau ** 2) * U)/(np.pi * np.sin(np.deg2rad(i)) * np.abs(Ux))\n",
    "    # return\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291148e0-a0db-424a-8538-1cdeb2f32546",
   "metadata": {},
   "source": [
    "Convert the impact probability per revolution $P$ into an impact probability per year $P_t$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ffc3ec-591f-474b-8350-5b8e77368945",
   "metadata": {},
   "outputs": [],
   "source": [
    "impactor_pts = np.array([get_P(a, e, i)/(a ** (3./2.)) for a, e, i in zip(impactor_as, impactor_es, impactor_is)]).T\n",
    "true_pts = np.array([get_P(a, e, i)/(a ** (3./2.)) for a, e, i in zip(true_as, true_es, true_is)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39674fa8-f958-4d0a-9f51-1db53dcb7e21",
   "metadata": {},
   "source": [
    "Filter impactors with `nan` impact probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904f547-3873-4ee7-a33b-58051600bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "impactor_pts_nonnan = list()\n",
    "impactor_qs_nonnan = list()\n",
    "\n",
    "for i in range(len(impactor_pts)):\n",
    "    non_nan_mask = ~np.isnan(impactor_pts[i]) # mask of non nan impact probabilities for that impactor\n",
    "    # filter out nan impact probabilities for that impactor\n",
    "    impactor_pts_nonnan_i = impactor_pts[i][non_nan_mask]\n",
    "    impactor_qs_nonnan_i = impactor_qs[i][non_nan_mask]\n",
    "    # and append to the list\n",
    "    impactor_pts_nonnan.append(impactor_pts_nonnan_i)\n",
    "    impactor_qs_nonnan.append(impactor_qs_nonnan_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86335188-54fd-433c-bb5e-0248de7512a1",
   "metadata": {},
   "source": [
    "Define function to filter clones with too low impact probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab2307-cbfb-441e-b8b7-c557862b9c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_clones(pts, qs, n_bins=20, lower=0.025, upper=0.975):\n",
    "    \"\"\"\n",
    "    Filters clones with too low impact probability for their respective bin\n",
    "\n",
    "    Takes as input pts, an array of annual impact probabilities, and qs, an array of corresponding q values\n",
    "\n",
    "    returns a tuple (pts_filtered, qs_filtered) which are the arrays pts and qs filtered to remove clones with too low impact probability\n",
    "\n",
    "    n_bins is the number of bins, lower is the lower bound of the confidence interval, upper is the upper bound of the confidence interval\n",
    "    \"\"\"\n",
    "    bin_lbs = np.linspace(0., 1. - 1./n_bins, n_bins)\n",
    "    # define filtered pts and qs\n",
    "    pts_filtered = np.array([])\n",
    "    qs_filtered = np.array([])\n",
    "\n",
    "    for bin_lb in bin_lbs:\n",
    "        # generate a mask to filter only for clones within that q bin\n",
    "        q_mask = (qs > bin_lb) & (qs < (bin_lb + 1./n_bins))\n",
    "        # get all pt values and q values of clones within that bin\n",
    "        bin_pts = pts[q_mask]\n",
    "        bin_qs = qs[q_mask]\n",
    "        # get 95% quantile of the impact probabilities within that q bin if nonempty, else just return the empty array\n",
    "        if bin_qs.size:\n",
    "            ci_lower, ci_upper = np.quantile(bin_pts, q=(lower, upper))\n",
    "            # now generate a mask to filter only for clones that fall within the 95% quantiles for impact probability\n",
    "            impact_prob_mask = ((bin_pts > ci_lower) & (bin_pts < ci_upper))\n",
    "            # finally, apply the impact probability mask to the pt and q values of clones within the q bin\n",
    "            filtered_bin_pts = bin_pts[impact_prob_mask]\n",
    "            filtered_bin_qs = bin_qs[impact_prob_mask]\n",
    "        else:\n",
    "            filtered_bin_pts = bin_pts\n",
    "            filtered_bin_qs = bin_qs\n",
    "        # and save the qs and probabilities for the bin\n",
    "        pts_filtered = np.append(pts_filtered, filtered_bin_pts)\n",
    "        qs_filtered = np.append(qs_filtered, filtered_bin_qs)\n",
    "        \n",
    "    # and finally return pts and qs\n",
    "    return (pts_filtered, qs_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381a93f9-fb50-45a4-9c84-f2029ba9816d",
   "metadata": {},
   "source": [
    "And try filtering using $20$ bins (a bin size of $0.025$ AU) and plot the differences when considering **all impactors**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6021f2fb-462a-4143-a25e-71f43993f8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter using 20 bins\n",
    "impactor_pts_flat_filtered, impactor_qs_flat_filtered = filter_clones(np.concatenate(impactor_pts_nonnan), np.concatenate(impactor_qs_nonnan), \n",
    "                                                            n_bins=20, lower=0.005, upper=0.995)\n",
    "\n",
    "# PLOTTING:\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.scatter(impactor_pts, impactor_qs, s=0.07, c='r')\n",
    "plt.scatter(impactor_pts_flat_filtered, impactor_qs_flat_filtered, s=0.07, c='k')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Annual impact probability'), plt.ylabel('q')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9ce0cb-d9a4-4d18-ba22-3f59d7e3a9c2",
   "metadata": {},
   "source": [
    "Then plot a histogram of the $\\log_{10}$ annual impact probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e054a-23c9-4136-ad93-32f3012e36b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=15)\n",
    "# convert to lgo to make more readable\n",
    "plt.figure(figsize = (12, 8))\n",
    "# plot histogram\n",
    "plt.hist(np.log10(impactor_pts_flat_filtered), bins='auto', density=True, color='k')\n",
    "# label axes\n",
    "plt.xlabel(r'$\\log_{10}\\,$impact probability per year'), plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d31ecd7-fc0c-44a1-8e90-eff3808145fd",
   "metadata": {},
   "source": [
    "Then plot a histogram comparing the weighted and unweighted filtered $q$ values for the cloned impactors as well as a histogram of the $q$ values for the cloned impactors, weighting by impact probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838320b4-5c4e-492d-b77f-78ac7611ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.hist(impactor_qs_flat_filtered, bins=25, color = 'red', label = 'Weighted Impactor', density=True, \n",
    "         weights=1./impactor_pts_flat_filtered, alpha=0.9)\n",
    "plt.hist(impactor_qs_flat_filtered, bins=25, color = 'orange', label = 'Unweighted Impactor', density=True, \n",
    "         weights=None, alpha=0.7)\n",
    "# plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e906e8d-f7bd-4b64-8d7a-32abcc117ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semi major axes of earth and venus\n",
    "a_earth = 1.\n",
    "a_venus = 0.72333199\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.hist(impactor_qs_flat_filtered, bins=25, color = 'red', label = 'Weighted Impactor', density=True, \n",
    "         weights=1./impactor_pts_flat_filtered, alpha=0.9)\n",
    "plt.hist(mag_limited_qs, bins=20, color='dodgerblue', label = 'CSS', density=True, \n",
    "         weights=None, alpha=0.7)\n",
    "# plt.yscale('log')\n",
    "plt.axvline(a_earth, color = 'k')\n",
    "plt.axvline(a_venus, color = 'k')\n",
    "plt.xlim(left=0., right=1.3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326368a-e289-4728-913a-17ae543d8fc0",
   "metadata": {},
   "source": [
    "Now try plotting $14$ histograms, one for each of the decameter objects:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf053d28-3792-4904-9a3b-5676e714ebde",
   "metadata": {},
   "source": [
    "Filtering for impact probability per impactor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d927e-d3ac-441c-8779-710b2ca29b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "impactor_pts_filtered = list()\n",
    "impactor_qs_filtered = list()\n",
    "\n",
    "for i in range(len(impactor_pts_nonnan)):\n",
    "    # filter clones for that impactor\n",
    "    impactor_pts_filtered_i, impactor_qs_filtered_i = filter_clones(impactor_pts[i], impactor_qs[i], n_bins=20)\n",
    "    # append to list\n",
    "    impactor_pts_filtered.append(impactor_pts_filtered_i)\n",
    "    impactor_qs_filtered.append(impactor_qs_filtered_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648b04d-0ec5-4a6e-9e8e-af88c051cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING:\n",
    "plt.figure(figsize = (12, 8))\n",
    "# plt.scatter(impactor_pts, impactor_qs, s=0.07, c='k')\n",
    "for i in range(len(impactor_pts_filtered)):\n",
    "    plt.scatter(impactor_pts_filtered[i], impactor_qs_filtered[i], s=0.5)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Annual impact probability'), plt.ylabel('q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7f7d7-cce5-456b-adb2-81531ea6a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING:\n",
    "plt.figure(figsize = (12, 8))\n",
    "# plt.scatter(impactor_pts, impactor_qs, s=0.07, c='k')\n",
    "for i in range(len(impactor_qs_filtered)):\n",
    "    plt.hist(impactor_qs_filtered[i], bins='auto', alpha=0.7, label =i)\n",
    "plt.xlabel('q'), plt.ylabel('Number')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21a4f1-3568-4fd1-b7a8-9904bf51a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTTING:\n",
    "plt.figure(figsize = (12, 8))\n",
    "# plt.scatter(impactor_pts, impactor_qs, s=0.07, c='k')\n",
    "for i in range(len(impactor_pts_filtered)):\n",
    "    plt.hist(np.log10(impactor_pts_filtered[i]), bins='auto', alpha=0.7, label = i)\n",
    "plt.xlabel('log10 Annual impact probability'), plt.ylabel('Number')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88f0b64-c3a9-4cd8-a37d-0fc0408ced66",
   "metadata": {},
   "source": [
    "Plot histograms of all $14$ impactors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc758926-26b6-471f-9afc-d6de67ebfd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.hist(np.concatenate(impactor_qs_filtered), bins=20, color='k', label = 'Weighted Unflattened', density=False, \n",
    "         weights=1./np.concatenate(impactor_pts_filtered), alpha=0.5)\n",
    "for i in range(len(impactor_pts_filtered)):\n",
    "    plt.hist(impactor_qs_filtered[i], bins=20, density=False, weights=1./impactor_pts_filtered[i], alpha=0.7, label = i)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9122f-4b69-49fc-8fa5-439eb0e6369b",
   "metadata": {},
   "source": [
    "Compare filtering over flattened to unflattened:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c70b4d-8246-4ad7-adec-94e0614cb1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.hist(impactor_qs_flat_filtered, bins=25, color = 'red', label = 'Weighted Impactor Flattened', density=True, \n",
    "         weights=1./impactor_pts_flat_filtered, alpha=0.9)\n",
    "plt.hist(np.concatenate(impactor_qs_filtered), bins=25, color='green', label = 'Weighted Impactor Unflattened', density=True, \n",
    "         weights=1./np.concatenate(impactor_pts_filtered), alpha=0.7)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca53b2-65c7-4789-a013-b81641f6b6f4",
   "metadata": {},
   "source": [
    "Weighted K-S test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d065499-52f8-472e-9e7c-5a8292a354fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import distributions\n",
    "\n",
    "def ks_weighted(data1, data2, wei1, wei2, alternative='two-sided'):\n",
    "    ix1 = np.argsort(data1)\n",
    "    ix2 = np.argsort(data2)\n",
    "    data1 = data1[ix1]\n",
    "    data2 = data2[ix2]\n",
    "    wei1 = wei1[ix1]\n",
    "    wei2 = wei2[ix2]\n",
    "    data = np.concatenate([data1, data2])\n",
    "    cwei1 = np.hstack([0, np.cumsum(wei1)/sum(wei1)])\n",
    "    cwei2 = np.hstack([0, np.cumsum(wei2)/sum(wei2)])\n",
    "    cdf1we = cwei1[np.searchsorted(data1, data, side='right')]\n",
    "    cdf2we = cwei2[np.searchsorted(data2, data, side='right')]\n",
    "    d = np.max(np.abs(cdf1we - cdf2we))\n",
    "    # calculate p-value\n",
    "    n1 = data1.shape[0]\n",
    "    n2 = data2.shape[0]\n",
    "    m, n = sorted([float(n1), float(n2)], reverse=True)\n",
    "    en = m * n / (m + n)\n",
    "    if alternative == 'two-sided':\n",
    "        prob = distributions.kstwo.sf(d, np.round(en))\n",
    "    else:\n",
    "        z = np.sqrt(en) * d\n",
    "        # Use Hodges' suggested approximation Eqn 5.3\n",
    "        # Requires m to be the larger of (n1, n2)\n",
    "        expt = -2 * z**2 - 2 * z * (m + 2*n)/np.sqrt(m*n*(m+n))/3.0\n",
    "        prob = np.exp(expt)\n",
    "    return d, prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9c7bcc-1a92-4a2e-b312-d163086dc3ad",
   "metadata": {},
   "source": [
    "Computing 1-D K-S test for unweighted and weighted $q$ values and compare unweighted result to default `scipy` K-S test implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05541c2-9218-478e-90b1-869e89600d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unweighted:', ks_weighted(impactor_qs_flat_filtered, np.array(mag_limited_qs), \n",
    "                  np.ones(len(impactor_qs_flat_filtered)), np.ones(len(np.array(mag_limited_qs))))) \n",
    "print('Weighted:', ks_weighted(impactor_qs_flat_filtered, np.array(mag_limited_qs), \n",
    "                  1./impactor_pts_flat_filtered, np.ones(len(np.array(mag_limited_qs)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c43cb1-7402-43c4-a77c-5c5defab790b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ks_2samp(impactor_qs_flat_filtered, mag_limited_qs).statistic, scipy.stats.ks_2samp(impactor_qs_flat_filtered, mag_limited_qs).pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b9064-83ae-4ef6-b0c4-446899ea9bd1",
   "metadata": {},
   "source": [
    "### Comparing to CNEOS fireballs, 824 EFN fireballs and the EMCCD Meteors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855441ca-5e71-42c3-a16b-aa12e856652b",
   "metadata": {},
   "source": [
    "Read in all the impactors recorded on the CNEOS website and get the state vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e47781-4728-4b13-9d92-7cea2c470261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in cneos fireballs, dropping row if any of time, latitude, longitude, altitude, vx, vy or vz are nan\n",
    "cneos_fireballs_raw = pd.read_csv('cneos_fireball_data.csv').dropna(subset=['Peak Brightness Date/Time (UT)', \n",
    "                                                                            'Latitude (deg.)', 'Longitude (deg.)', \n",
    "                                                                            'Altitude (km)', 'vx', 'vy', 'vz'])\n",
    "# remove decameter-sized impactors\n",
    "cneos_fireballs_raw = cneos_fireballs_raw[cneos_fireballs_raw['Calculated Total Impact Energy (kt)'] <= 10.]\n",
    "# 304 fireballs in total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3d456-c8cf-4267-a2e4-b9df5d50dd3c",
   "metadata": {},
   "source": [
    "Convert `vx`, `vy`, `vz` to azimuth, zenith and speed and filter out events for which the zenith is not between $0$ and 90$ degrees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ed158-590c-465a-b461-365e60de7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rtd is constant\n",
    "rtd = np.pi/180  # rad/deg\n",
    "# get time, converting to the format used by pylig: '%Y%m%d-%H%M%S.%f'\n",
    "cneos_t = pd.to_datetime(cneos_fireballs_raw['Peak Brightness Date/Time (UT)'], format='mixed').dt.strftime('%Y%m%d-%H%M%S.%f')\n",
    "# get elevation\n",
    "cneos_elevation = cneos_fireballs_raw['Altitude (km)']\n",
    "\n",
    "# get x, y, z velocities, dropping nans\n",
    "cneos_vx, cneos_vy, cneos_vz = cneos_fireballs_raw.vx, cneos_fireballs_raw.vy, cneos_fireballs_raw.vz\n",
    "# convert to numerical long, lat, dropping nans\n",
    "cneos_latitude = (cneos_fireballs_raw['Latitude (deg.)'].str[:-1].astype(float) * (2 * (cneos_fireballs_raw['Latitude (deg.)'].str[-1:] == 'N') - 1))\n",
    "cneos_longitude = (cneos_fireballs_raw['Longitude (deg.)'].str[:-1].astype(float) * (2 * (cneos_fireballs_raw['Longitude (deg.)'].str[-1:] == 'E') - 1))\n",
    "\n",
    "# cneos_vx, cneos_vy, cneos_vz = -10.8, 1.2, -12.8\n",
    "# cneos_latitude = 59.8\n",
    "# cneos_longitude = 16.8\n",
    "\n",
    "# should get 242.771 azim, 16.6202 zen, 16.7905 speed\n",
    "\n",
    "# get vn, ve, vd from vx, vy, vz and lat, long\n",
    "cneos_vn = -cneos_vx * np.sin(cneos_latitude * rtd) * np.cos(cneos_longitude * rtd) - cneos_vy * np.sin(cneos_longitude * rtd) * np.sin(cneos_latitude * rtd) + cneos_vz * np.cos(cneos_latitude * rtd)\n",
    "cneos_ve = -cneos_vx * np.sin(cneos_longitude * rtd) + cneos_vy * np.cos(cneos_longitude * rtd)\n",
    "cneos_vd = -cneos_vx * np.cos(cneos_latitude * rtd) * np.cos(cneos_longitude * rtd) - cneos_vy * np.cos(cneos_latitude * rtd) * np.sin(cneos_longitude * rtd) - cneos_vz * np.sin(cneos_latitude * rtd)\n",
    "# get azimuth and zenith\n",
    "cneos_azim = np.arctan2(cneos_ve, cneos_vn)/rtd + 180.  # NOTE that arctan2 in excel is (x, y) but arctan2 in numpy is (y, x)!\n",
    "cneos_zen = np.arctan(np.sqrt(cneos_vn ** 2 + cneos_ve ** 2)/cneos_vd)/rtd\n",
    "# get total velocity\n",
    "cneos_v = np.sqrt(cneos_vx ** 2 + cneos_vy ** 2 + cneos_vz ** 2)\n",
    "\n",
    "# filter\n",
    "zen_mask = (0. < cneos_zen) & (cneos_zen < 90.)  # create mask for zenith between 0 and 90 degrees\n",
    "\n",
    "# filter v, t, lat, lon, elevation, azim, zenith by the mask\n",
    "cneos_v = cneos_v[zen_mask]\n",
    "cneos_t = cneos_t[zen_mask]\n",
    "cneos_latitude = cneos_latitude[zen_mask]\n",
    "cneos_longitude = cneos_longitude[zen_mask]\n",
    "cneos_elevation = cneos_elevation[zen_mask]\n",
    "cneos_azim = cneos_azim[zen_mask]\n",
    "cneos_zen = cneos_zen[zen_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba8f740-9a7e-48ab-9a6f-205d909c57e6",
   "metadata": {},
   "source": [
    "Import the calibrated fireball set and compute uncertainties for speed and radiant (this section is copied from `monte_carlo_granvik_model.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab91b1d-686f-4af6-80ad-8dd0c68fb252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip the first two rows after header and the last three rows since those are garbage\n",
    "# note that we want the usg-ground-based-comparison file to be in this form since we are removing absolute rows\n",
    "usg_ground_based_comparison = pd.read_csv('usg-ground-based-comparison/usg-ground-based-comparison_EDITED.csv', \n",
    "                                          sep = ',', skip_blank_lines=True).iloc[2:-3]  \n",
    "\n",
    "# forward fill Event and radiant, backward fill dv based on the xlsx file\n",
    "usg_ground_based_comparison['Event'] = usg_ground_based_comparison['Event'].fillna(method='ffill')\n",
    "usg_ground_based_comparison['Radiant Diff'] = usg_ground_based_comparison['Radiant Diff'].fillna(method='ffill')\n",
    "usg_ground_based_comparison['DV'] = usg_ground_based_comparison['DV'].fillna(method='bfill')\n",
    "\n",
    "# drop every other row since we don't need duplicate rows for event, radiant diff, or dv\n",
    "# however we want to keep the usg reported speed, zenith angle and other parameters (second row for each event), \n",
    "# so we start by dropping the first row rather than the second\n",
    "usg_ground_based_comparison = usg_ground_based_comparison.iloc[1::2]\n",
    "# strip whitespace from headers to avoid errors with indexing later\n",
    "usg_ground_based_comparison.columns = usg_ground_based_comparison.columns.str.strip()\n",
    "# sort dataframe alphabetically inplace\n",
    "usg_ground_based_comparison.sort_values('Event', inplace=True)\n",
    "\n",
    "# convert Speed, DV, Radiant Diff, Radiant Zenith Angle, Height, Begin Height, End Height, Length to numeric to avoid errors\n",
    "cols_to_convert = ['Speed', 'DV', 'Radiant Diff', 'Radiant Zenith Angle', 'Height', 'Begin Height (km)', 'End Height (km)', 'Length (km)']\n",
    "usg_ground_based_comparison[cols_to_convert] = usg_ground_based_comparison[cols_to_convert].apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "\n",
    "# convert date to datetime object\n",
    "usg_ground_based_comparison['Date'] = pd.to_datetime(usg_ground_based_comparison['Date'])\n",
    "\n",
    "# usg speed, dv, drad\n",
    "usg_speed = usg_ground_based_comparison['Speed']\n",
    "# speed and radiant uncertainties\n",
    "dv = usg_ground_based_comparison['DV']\n",
    "drad = usg_ground_based_comparison['Radiant Diff']\n",
    "\n",
    "#### KDES\n",
    "\n",
    "# Speed\n",
    "\n",
    "# parameters for the KDE fitting\n",
    "dv_ub = 8.  # upper bound for fitting kde\n",
    "dv_lb = -8.  # lower bound for fitting kde\n",
    "dv_bandwidth = 0.7  # this is finicky  # 0.8 used by default\n",
    "kernel_type = 'gaussian'  # use gaussian kernel\n",
    "\n",
    "# setting up the values\n",
    "values_dv = np.linspace(dv_lb, dv_ub, int(1e4))\n",
    "kde_dv = np.array(dv).reshape((len(dv), 1))\n",
    "kde_values_dv = values_dv.reshape(len(values_dv), 1)\n",
    "\n",
    "# doing the KDE fit\n",
    "kde_model_dv = KernelDensity(bandwidth=dv_bandwidth, kernel=kernel_type)\n",
    "kde_model_dv.fit(kde_dv)\n",
    "\n",
    "# parameters for the KDE fitting\n",
    "drad_lb = 0.  # upper bound for fitting kde\n",
    "drad_ub = 2.  # lower bound for fitting kde\n",
    "log_drad_bandwidth = 0.15  # this is finicky  # 0.15 used by default\n",
    "\n",
    "# Radiant\n",
    "\n",
    "# setting up the values\n",
    "log_drad = np.log10(drad)  # convert to log\n",
    "values_drad = np.linspace(drad_lb, drad_ub, int(1e4))\n",
    "kde_log_drad = np.array(log_drad).reshape((len(log_drad), 1))\n",
    "kde_values_log_drad = values_drad.reshape(len(values_drad), 1)\n",
    "\n",
    "# kde fit\n",
    "kde_model_log_drad = KernelDensity(bandwidth=log_drad_bandwidth, kernel=kernel_type)  # use the same kernel as for the dv kde\n",
    "kde_model_log_drad.fit(kde_log_drad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb31b79-37e9-4d75-b446-c13787cf6d85",
   "metadata": {},
   "source": [
    "Then for each event, get $100$ Monte Carlo samples, compute the orbital parameters for each Monte Carlo samples, and save it to an array (also copied from `monte_carlo_granvik_model.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a2f11-9095-4768-9cbf-5861e54358d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# list of orbital parameters we want to get (named the same as in the wmpl output, in that order\n",
    "orb_param_variables = ['a', 'e', 'i', 'peri', 'node', 'M']  \n",
    "# number of Monte Carlo samples for each event\n",
    "n_monte_carlo_samples = 100  # use 100 MC samples\n",
    "\n",
    "### HELPER FUNCTION to get the zenith/azimuth from the radiant uncertainty\n",
    "def get_zen_azim_diff(zen, azim, theta, n_samples):\n",
    "    \"\"\"\n",
    "    Get n_samples random new radiants that are a fixed angle theta away from the radiant given by (zen, azim) in 3D\n",
    "    Note that zenith angle is 90 degrees - azimuth angle\n",
    "\n",
    "    param zen: zenith angle, in degrees\n",
    "    param azim: azimuth angle, in degrees\n",
    "    param theta: angle from radiant that we want our new radiants to be, in degrees\n",
    "    param n_samples: number of new radiants we want\n",
    "\n",
    "    return zen_diff, azim_diff: tuple of arrays of floats of size n_samples each: \n",
    "    zen_diff and azim_diff are the new randomly generated radiant zenith/azimuth minus the given radiant zenith/azimuth\n",
    "    \"\"\"\n",
    "    new_zens_deg = np.array([np.nan])  # initialize this to get the while loop started\n",
    "\n",
    "    # this is a while loop to reject any zenith distance above 90 or below 0:\n",
    "    while not np.all((new_zens_deg > 0.) & (new_zens_deg < 90.)):\n",
    "        phis = np.random.uniform(0., 2 * np.pi, size=n_samples)  # these are the random directions we want our radiants to be in\n",
    "\n",
    "        # convert zenith and azimuth into cartesian coordinates x, y, z\n",
    "        v1 = np.array([np.sin(np.deg2rad(zen)) * np.cos(np.deg2rad(azim)), \n",
    "                       np.sin(np.deg2rad(zen)) * np.sin(np.deg2rad(azim)), \n",
    "                       np.cos(np.deg2rad(zen))])\n",
    "        # get random new normalized vectors that are an angle theta away from v1, following the procedure described here:\n",
    "        # https://math.stackexchange.com/questions/2057315/parameterising-all-vectors-a-fixed-angle-from-another-vector-in-3d\n",
    "        # standard basis vector to use for generating orthonormal basis\n",
    "        standard_basis_vec = np.array([1, 0, 0]) \n",
    "        # generate two basis vectors orthonormal to v1\n",
    "        # normalize v1 in case it isn't already\n",
    "        v1_normed = v1/np.linalg.norm(v1)\n",
    "        # generate v2\n",
    "        v2 = np.cross(v1_normed, standard_basis_vec)\n",
    "        # normalize it\n",
    "        v2_normed = v2/np.linalg.norm(v2)\n",
    "        # generate a normalized v3 from the normalized v1 and v2\n",
    "        v3_normed = np.cross(v1_normed, v2_normed)\n",
    "        # now generate new vectors with radiant difference scattered by phis using vectorized operations\n",
    "        new_radiants = v1_normed + np.tan(np.deg2rad(theta)) * (np.multiply.outer(np.cos(phis), v2_normed) + \n",
    "                                                                np.multiply.outer(np.sin(phis), v3_normed))\n",
    "        # normalize the new vectors\n",
    "        new_radiants_normed = new_radiants/np.linalg.norm(new_radiants, axis=1).reshape(-1, 1)\n",
    "        # convert from cartesian coordinates back to zenith/azimuth coordinates\n",
    "        xs, ys, zs = new_radiants_normed.T\n",
    "        # get zeniths from cartesian\n",
    "        new_zens = np.arccos(zs)\n",
    "        # get azimuths from cartesian\n",
    "        new_azims = np.arcsin(ys/np.sin(new_zens))\n",
    "        # convert to degrees\n",
    "        new_zens_deg = np.rad2deg(new_zens)\n",
    "        new_azims_deg = np.rad2deg(new_azims)\n",
    "        # first, manually convert the azimuths to the right quadrant since they can range from [0, 2pi] while zeniths can only be [0, pi/2]\n",
    "        # Q1\n",
    "        if 0. <= azim <= 90.:\n",
    "            new_azims_deg = new_azims_deg  # already correct\n",
    "        # Q2 and Q3:\n",
    "        if 90. < azim <= 270.:\n",
    "            new_azims_deg = np.abs(new_azims_deg - 180.)  # convert \n",
    "        # Q4:\n",
    "        if 270. < azim <= 360.:\n",
    "            new_azims_deg += 360  # convert\n",
    "        \n",
    "        # # finally, convert any negative azimuths back to [0, 360] degrees by adding 360 to any values less than 0\n",
    "        # new_azims_deg += 360 * (new_azims_deg < 0)\n",
    "        # # and convert any azimuths above 360 back down to [0, 360] as well\n",
    "        # new_azims_deg = np.mod(new_azims_deg, 360)\n",
    "            \n",
    "    # return zenith and azimuth difference\n",
    "    return new_zens_deg - zen, new_azims_deg - azim\n",
    "\n",
    "\n",
    "# draw a monte carlo sample and get the orbital parameters using WMPL\n",
    "def get_monte_carlo_orbital_parameters(kdes_state_vector_params, orb_param_variables=orb_param_variables):\n",
    "    \"\"\"\n",
    "    Samples a Monte Carlo sample from provided KDEs for speed and radiant difference, and gets \n",
    "    the orbital parameter values corresponding to each parameter in orb_param_variables from the state vector state_vector\n",
    "    \n",
    "    param kdes_state_vector_params: tuple of (dv_kde, drad_kde, v, t, a, o, e, azim, alt):\n",
    "    dv_kde is the speed uncertainty kde: should be a scikit-learn KernelDensity object\n",
    "    log_drad_kde is the log10-radiant uncertainty kde: should be a scikit-learn KernelDensity object\n",
    "    v is the velocity in km/s\n",
    "    t is the time of the event in the form that the wmpl.Trajectory.Orbit function takes in, '%Y%m%d-%H%M%S.%f'\n",
    "    a is the latitude, in degrees N\n",
    "    o is the longitude, in degrees E\n",
    "    e is the elevation of the event, in km\n",
    "    azim is the radiant azimuth, in degrees\n",
    "    zen is the zenith distance (90 minus the altitude angle), in degrees\n",
    "\n",
    "    returns a 1-D array of parameter values corresponding to the parameters in orb_param_variables, in the same order\n",
    "    \"\"\"\n",
    "    orb_param_array = np.array([])  # initialize orb_param_array as empty array so it doesn't have a size to start\n",
    "\n",
    "    # unpack kdes and state_vector_params\n",
    "    dv_kde, log_drad_kde, v, t, a, o, e, azim, zen = kdes_state_vector_params  # get the kdes and state vector parameters\n",
    "    # converting from zenith angle to altitude angle\n",
    "    alt = 90 - zen\n",
    "    \n",
    "    #### THIS IS A WHILE LOOP AS A SAFEGUARD BECAUSE IT SEEMS LIKE SOMETIMES WMPL DOESN'T RETURN THE ORBITAL ELEMENTS AT ALL\n",
    "    # so if orbital parameter array is empty (no orbital elements returned) then we repeat this\n",
    "    while not orb_param_array.size:\n",
    "        np.random.seed(None)  # explicitly reset the seed since the sampler for sklearn doesn't seem to be sampling randomly otherwise\n",
    "        # get monte carlo sample of the speed and log-radiant uncertainty\n",
    "        dv = dv_kde.sample(n_samples=1).flatten()  # get a single dv mc sample\n",
    "        log_drad = log_drad_kde.sample(n_samples=1).flatten()  # get a single log drad mc sample\n",
    "\n",
    "        # convert the log-radiant uncertainty into zenith and azimuth uncertainty using the helper function\n",
    "        # with the restrictions that zenith must be between [0, 90] degrees\n",
    "        dzen, dazim = get_zen_azim_diff(zen, azim, 10 ** log_drad, n_samples=1)  # log10-radiant so convert back to radiant\n",
    "        # difference in altitude is negative of difference in zenith since it's 90 - zenith\n",
    "        # so zen1 - zen2 = (90 - alt1) - (90 - alt2) = alt2 - alt1\n",
    "        dalt = -dzen\n",
    "        # adding the uncertainty in velocity to the velocity vector of the event\n",
    "        # and adding the uncertainty in zenith angle and azimuth angle to the radiant\n",
    "        # convert the singleton arrays to floats as well\n",
    "        new_v = v + dv[0]\n",
    "        new_alt = alt + dalt[0]\n",
    "        # azimuth is modulo 360 to keep it between 0 and 360 degrees\n",
    "        new_azim = (azim + dazim[0]) % 360\n",
    "        # get the output from the wmpl.Trajectory.Orbit script\n",
    "        output = subprocess.run(['python3', '-m', 'wmpl.Trajectory.Orbit', '-v', f'{new_v}', '-t', f'{t}', '-a', f'{a}', \n",
    "                                 '-o', f'{o}', '-e', f'{e}', '--azim', f'{new_azim}', '--alt', f'{new_alt}', \n",
    "                                 '--vrotcorr', '--statfixed'], capture_output=True)\n",
    "        # get the orbital parameters from the output in the order of orb_param_variables\n",
    "        \n",
    "        # orb_param_list = [elem for elem in list(map(str.strip, output.stdout.decode('utf-8').split('\\n'))) \n",
    "        #               if elem.startswith(tuple(orb_param_variables))]\n",
    "        # orb_param_array = [np.float64(string) for param in orb_param_list \n",
    "        #                    for string in param.split() if string.lstrip('-').replace('.', '').isdigit() or string =='nan']]\n",
    "        \n",
    "        orb_param_array = np.array([np.float64(string) for param in [elem for elem in \n",
    "                                                                     list(map(str.strip, output.stdout.decode('utf-8').split('\\n'))) \n",
    "                                                                     if elem.startswith(tuple(orb_param_variables))] \n",
    "                                    for string in param.split() if string.lstrip('-').replace('.', '').isdigit() or string =='nan'])\n",
    "    \n",
    "    # return the orbital parameter array\n",
    "    return orb_param_array\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39d1a2-d3bc-4b94-bbd9-739f457f2d7f",
   "metadata": {},
   "source": [
    "Optional: Run them through Pylig to get the orbital parameter values, using multiprocessing to parallelize it. This takes about $2$ hours to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b864fd0-2c8c-4b45-81f1-4c04b11a7214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define list of all CNEOS state vectors\n",
    "# cneos_sv_list = list(np.array([cneos_v, cneos_t, cneos_latitude, cneos_longitude, cneos_elevation, cneos_azim, cneos_zen]).T)\n",
    "# # define array to save the orbital parameters to\n",
    "# orb_param_array_all_cneos_fireballs = np.zeros((len(cneos_sv_list), n_monte_carlo_samples, len(orb_param_variables)))\n",
    "\n",
    "# # perform the Monte Carlo cloning and compute the orbital parameters for each cloned state vector\n",
    "# for i, sv in tqdm(enumerate(cneos_sv_list), total=len(cneos_sv_list)):\n",
    "#     # unpack\n",
    "#     v, t, a, o, e, azim, zen = sv\n",
    "#     # repack with the kde models and repeat 100 times to clone it\n",
    "#     repacked_sv = np.repeat([[kde_model_dv, kde_model_log_drad, v, t, a, o, e, azim, zen]], n_monte_carlo_samples, axis=0)\n",
    "#     # now compute orbital parameters from the state vectors for each Monte Carlo sample using multiprocessing:\n",
    "#     pool = multiprocessing.Pool(multiprocessing.cpu_count() - 1)  # avoids locking up CPU\n",
    "#     # convert cloned state vectors with orbital parameters\n",
    "#     orb_param_array_single_cneos_fireball = np.array(list(pool.imap(get_monte_carlo_orbital_parameters, repacked_sv)))\n",
    "#     # add the orbital parameter array for the single fireball to the array for all fireballs\n",
    "#     orb_param_array_all_cneos_fireballs[i] = orb_param_array_single_cneos_fireball"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b28ef6-4aba-4ea1-a9bf-f276a9952e88",
   "metadata": {},
   "source": [
    "Optional: Save file as Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee28c4f-9ebf-4e77-9e4e-015e77b5bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('orb_param_array_all_cneos_fireballs.npy', orb_param_array_all_cneos_fireballs)  # save as a .npy file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdfd27b-67b7-495a-a06f-c356088fa393",
   "metadata": {},
   "source": [
    "Optional: Load file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e8f843-a7d1-441c-9f2a-6d838dd58fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "orb_param_array_all_cneos_fireballs = np.load('orb_param_array_all_cneos_fireballs.npy')  # load .npy file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6d82b-47be-4342-ae45-cc3f0f8decc9",
   "metadata": {},
   "source": [
    "Now read in the $824$ EFN fireballs from Borovicka et al. 2022 as well as the EMCCD data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb955e8-5836-40df-b95f-e200eb45578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in EFN fireballs\n",
    "\n",
    "efn_fireballs = pd.read_fwf('catalog.dat', header=None).drop(columns=[81, 85]) # drop column 81 as well since it's having trouble reading phaethon\n",
    "efn_fireballs.columns = ['code', 'obs_date', 'obs_time', 'e_obs_time', 'jd', \n",
    "                        'lsun', 'lam-beg', 'phi-beg', 'h-beg', 'lam-end', \n",
    "                        'phi-end', 'h-end', 'lam-avg', 'phi-avg', 'h-avg', \n",
    "                        'lam-max', 'phi-max', 'h-max', 'length', 'dur', \n",
    "                        'azim', 'zendis', 'radeg', 'e_radeg', 'dedeg', \n",
    "                        'e_dedeg', 'vinf', 'e_vinf', 'vmax', 'vter', \n",
    "                        'hvter', 'rageo', 'e_rageo', 'degeo', 'e_degeo', \n",
    "                        'lgeo-lsun', 'bgeo', 'vgeo', 'e_vgeo', 'lhel', \n",
    "                        'e_lhel', 'bhel', 'e_bhel', 'vhel', 'e_vhel', \n",
    "                        'ax', 'e_ax', 'ecc', 'e_ecc', 'perih', \n",
    "                        'e_perih', 'aph', 'e_aph', 'inc', 'e_inc', \n",
    "                        'omg', 'e_omg', 'nod', 'e_nod', 'lper', \n",
    "                        'e_lper', 'tyear', 'tmonth', 'tday', 'e_tday', \n",
    "                        'per', 'e_per', 'tiss', 'e_tiss', 'mag', \n",
    "                        'energy', 'mass', 'termass', 'pe', 'type', \n",
    "                        'pres', 'hpres', 'pf', 'pf-class', 'shower', \n",
    "                        'object', 'ncam', 'mindist', 'spectrum']\n",
    "\n",
    "# Read in EMCCD data\n",
    "\n",
    "# import kb_and_kc_calc\n",
    "# emccd_meteors = kb_and_kc_calc.extract_from_table()\n",
    "emccd_meteors = pd.read_json('solution_table.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6387b9-2990-49e3-bd7b-9142f04f5e41",
   "metadata": {},
   "source": [
    "Get the $a$, $e$, $i$, $q$ values and perform the same impact probability weighting for the CNEOS fireballs, EFN fireballs and the EMCCD data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea19197a-68b4-422d-ac8a-95cc0287dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emccd_meteors.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0532cde-30ca-43ae-b34c-a20293aa5503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a, e, i, q for CNEOS fireballs and flatten\n",
    "cneos_as, cneos_es, cneos_is, cneos_peris, cneos_nodes, cneos_Ms = orb_param_array_all_cneos_fireballs.T\n",
    "cneos_qs = cneos_as * (1. - cneos_es)\n",
    "cneos_qs_flat = cneos_qs.reshape(-1)\n",
    "\n",
    "# for EFN and EMCCD fireballs\n",
    "efn_as, efn_es, efn_is, efn_qs = np.array(efn_fireballs[['ax', 'ecc', 'inc', 'perih']]).T\n",
    "emccd_as, emccd_es, emccd_is, emccd_omegas, emccd_nodes, emccd_ms, emccd_qs = np.array(emccd_meteors[['a', 'e', 'i', 'omega', 'asc_node', 'mean_anomaly', 'q']]).T\n",
    "\n",
    "# and get the CNEOS, EFN and EMCCD annual impact probabilities\n",
    "cneos_pts = np.array([get_P(a, e, i)/(a ** (3./2.)) for a, e, i in zip(cneos_as, cneos_es, cneos_is)])\n",
    "efn_pts = np.array([get_P(a, e, i)/(a ** (3./2.)) for a, e, i in zip(efn_as, efn_es, efn_is)])\n",
    "emccd_pts = np.array([get_P(a, e, i)/(a ** (3./2.)) for a, e, i in zip(emccd_as, emccd_es, emccd_is)])\n",
    "\n",
    "cneos_pts_flat = cneos_pts.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024f245-0f97-48dc-839f-de2fbc86ff23",
   "metadata": {},
   "source": [
    "Perform the same filtering for `nan` values and impact probabilities for the CNEOS, EFN and EMCCD data as for the decameter data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21463dae-a65f-482e-8252-c94773c826de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cneos_non_nan_mask = ~np.isnan(cneos_pts_flat)\n",
    "efn_non_nan_mask = ~np.isnan(efn_pts)  # mask of all non-nan impact probabilities\n",
    "emccd_non_nan_mask = ~np.isnan(emccd_pts)\n",
    "\n",
    "# filter out nan impact probabilities from flat CNEOS, EFN and EMCCD data\n",
    "cneos_pts_nonnan = cneos_pts_flat[cneos_non_nan_mask]\n",
    "cneos_qs_nonnan = cneos_qs_flat[cneos_non_nan_mask]\n",
    "\n",
    "efn_pts_nonnan = efn_pts[efn_non_nan_mask]\n",
    "efn_qs_nonnan = efn_qs[efn_non_nan_mask]\n",
    "\n",
    "emccd_pts_nonnan = emccd_pts[emccd_non_nan_mask]\n",
    "emccd_qs_nonnan = emccd_qs[emccd_non_nan_mask]\n",
    "\n",
    "# filter impact probabilities using 20 bins\n",
    "cneos_pts_filtered, cneos_qs_filtered = filter_clones(cneos_pts_nonnan, cneos_qs_nonnan, n_bins=20)\n",
    "efn_pts_filtered, efn_qs_filtered = filter_clones(efn_pts_nonnan, efn_qs_nonnan, n_bins=12, lower=0.05, upper=0.95)  # fiddle with this, it's tough to make this work...\n",
    "emccd_pts_filtered, emccd_qs_filtered = filter_clones(emccd_pts_nonnan, emccd_qs_nonnan, n_bins=20, lower=0.05, upper=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a716e8b-015f-4b63-9660-5b6a45b7a84c",
   "metadata": {},
   "source": [
    "Plot probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785873b-709d-4496-b4e4-d6608d3be7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=15)\n",
    "# convert to lgo to make more readable\n",
    "plt.figure(figsize = (12, 8))\n",
    "# plot histogram\n",
    "plt.hist(np.log10(efn_pts_filtered), bins='auto', density=True, label = 'EFN', color='orange', alpha=0.9)\n",
    "plt.hist(np.log10(emccd_pts_filtered), bins='auto', density=True, label = 'EMCCD', color='black', alpha=0.7)\n",
    "plt.hist(np.log10(cneos_pts_filtered), bins='auto', density=True, label = 'CNEOS', color='green', alpha=0.7)\n",
    "# label axes\n",
    "plt.xlabel(r'$\\log_{10}\\,$impact probability per year'), plt.ylabel('Density')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb4372-4549-40b8-87f7-b50f2d7b90d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.scatter(emccd_pts, emccd_qs, s=0.07, c='black', label = 'EMCCD', alpha=0.6)\n",
    "# plt.scatter(efn_pts, efn_qs, s=5, c='orange', label = 'EFN', alpha=0.9)\n",
    "# plt.scatter(cneos_pts, cneos_qs, s=0.1, c='green', label = 'CNEOS', alpha = 0.9)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Annual impact probability'), plt.ylabel('q')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0719a-c678-4f35-ad6e-dd06b1801b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.scatter(emccd_pts_filtered, emccd_qs_filtered, s=0.07, c='black', label = 'EMCCD', alpha=0.6)\n",
    "# plt.scatter(efn_pts_filtered, efn_qs_filtered, s=5, c='orange', label = 'EFN', alpha=0.9)\n",
    "# plt.scatter(cneos_pts_filtered, cneos_qs_filtered, s=0.1, c='green', label = 'CNEOS', alpha = 0.9)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Annual impact probability'), plt.ylabel('q')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8bd3d-7294-480e-861a-47df3331d903",
   "metadata": {},
   "source": [
    "And plot a histogram of the CSS observations against the weighted decameter impactors, weighted CNEOS fireballs, weighted EFN fireballs, weighted EMCCD meteors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e924281f-c399-45c0-997a-e54359db37f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cneos_qs_filtered, bins=25, color='green', label = 'Weighted CNEOS', density=True, \n",
    "         weights=1./cneos_pts_filtered, alpha=0.7)\n",
    "plt.axvline(a_earth, color='k')\n",
    "plt.axvline(a_venus, color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703d4dd-c0e7-43cd-9732-07c44e099430",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(efn_qs_filtered, bins=25, color='orange', label = 'Weighted EFN', density=True, \n",
    "         weights=None, alpha=0.7)\n",
    "plt.axvline(a_earth, color='k')\n",
    "plt.axvline(a_venus, color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c68881-572d-44cb-b830-62a550274885",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(emccd_qs_filtered, bins=25, color='black', label = 'Unweighted EMCCD', density=False, \n",
    "         weights=None, alpha=0.7)\n",
    "plt.axvline(a_earth, color='k')\n",
    "plt.axvline(a_venus, color='k')\n",
    "plt.ylim(0, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86295e-dff7-4fa4-ab07-7db9d5fde44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(emccd_qs_filtered, bins=25, color='black', label = 'Weighted EMCCD', density=True, \n",
    "         weights=1./emccd_pts_filtered, alpha=0.7)\n",
    "plt.axvline(a_earth, color='k')\n",
    "plt.axvline(a_venus, color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab3efe-537f-4597-a6bc-2a9197c978d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "# CSS\n",
    "plt.hist(mag_limited_qs, bins=25, color='dodgerblue', label = 'CSS', density=True, \n",
    "         weights=None, alpha=0.7)\n",
    "# Decameter impactors\n",
    "plt.hist(impactor_qs_flat_filtered, bins=25, color = 'red', label = 'Weighted Decameter', density=True, \n",
    "         weights=1./impactor_pts_flat_filtered, alpha=0.9)\n",
    "# CNEOS Fireballs\n",
    "# plt.hist(cneos_qs[cneos_non_nan_mask], bins=25, color='green', label = 'Unweighted CNEOS Fireballs', density=True, alpha=0.7)\n",
    "plt.hist(cneos_qs_filtered, bins=25, color='green', label = 'Weighted CNEOS', density=True, \n",
    "         weights=1./cneos_pts_filtered, alpha=0.7)\n",
    "# EFN Fireballs\n",
    "# plt.hist(efn_qs[efn_non_nan_mask], bins=25, color='green', label = 'Unweighted EFNFireballs', density=True, alpha=0.7)\n",
    "plt.hist(efn_qs_filtered, bins=25, color = 'orange', label = 'Weighted EFN', density=True, \n",
    "         weights=1./efn_pts_filtered, alpha=0.7)\n",
    "# EMCCD Meteors\n",
    "# plt.hist(emccd_qs[emccd_non_nan_mask], bins=25, color='green', label = 'Unweighted EMCCD Meteors', density=True)\n",
    "plt.hist(emccd_qs_filtered, bins=25, color = 'black', label = 'Weighted EMCCD', density=True, \n",
    "         weights=1./emccd_pts_filtered, alpha=0.6)\n",
    "\n",
    "# plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.axvline(a_earth, color = 'k')\n",
    "plt.axvline(a_venus, color = 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeebf59f-79fe-4fff-b4a6-9bdb44027e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mag_limited_qs), len(impactor_qs_flat_filtered), len(cneos_qs_filtered), len(efn_qs_filtered), len(emccd_qs_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3387f3-41a6-493b-b84d-5190de318944",
   "metadata": {},
   "source": [
    "Plot heatmaps of $q$ vs. all other orbital elements for the EMCCD data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9f02a8-5acf-4c8b-8cbf-d8d1e7f4a03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "# plt.figure(figsize = (8, 8))\n",
    "# plt.scatter(emccd_qs, emccd_es, s=0.07, color='r')\n",
    "max_e = 2.  # set the maximum e value\n",
    "n_bins = 200\n",
    "\n",
    "plt.hist2d(emccd_qs, emccd_es, norm=mpl.colors.LogNorm(), bins=n_bins, range=[[0., np.max(emccd_qs)], [0., max_e]])\n",
    "plt.xlabel('q'), plt.ylabel('e')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b88450-7603-48ff-af51-40b18fb00633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize = (8, 8))\n",
    "plt.hist2d(emccd_qs, emccd_is, norm=mpl.colors.LogNorm(), bins=n_bins)\n",
    "# plt.scatter(emccd_qs, emccd_is, s=0.07, color='r')\n",
    "plt.xlabel('q'), plt.ylabel('i')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d50ab-710c-4eaa-abf8-3970d4797fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask all the nan mean anomalies\n",
    "mean_anomaly_mask = ~np.isnan(emccd_ms)\n",
    "\n",
    "# plot, masking the nan mean anomalies\n",
    "# plt.figure(figsize = (8, 8))\n",
    "plt.hist2d(emccd_qs[mean_anomaly_mask], np.mod(emccd_ms[mean_anomaly_mask] + 180., 360.) - 180, norm=mpl.colors.LogNorm(), bins=n_bins)\n",
    "# plt.scatter(emccd_qs, emccd_ms, s=0.07, color='r')\n",
    "plt.xlabel('q'), plt.ylabel('M')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced989c9-931c-48a7-8d76-1bc8b16d6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot, masking the nan mean anomalies\n",
    "# plt.figure(figsize = (8, 8))\n",
    "plt.hist2d(emccd_qs, emccd_nodes, norm=mpl.colors.LogNorm(), bins=n_bins)\n",
    "# plt.scatter(emccd_qs, emccd_ms, s=0.07, color='r')\n",
    "plt.xlabel('q'), plt.ylabel('M')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
